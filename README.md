# ï¿½ Data Engineering Portfolio - Python ETL Projects

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)](https://pandas.pydata.org/)
[![Jupyter](https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white)](https://jupyter.org/)
[![SQLite](https://img.shields.io/badge/SQLite-07405E?style=for-the-badge&logo=sqlite&logoColor=white)](https://www.sqlite.org/)

## ï¿½ğŸ“‹ DescripciÃ³n del Proyecto

Portfolio completo de **IngenierÃ­a de Datos** que demuestra dominio en procesos **ETL (Extract, Transform, Load)** utilizando diferentes tecnologÃ­as y metodologÃ­as. Este repositorio implementa la **Arquitectura Medallion** (Bronze â†’ Silver â†’ Gold) y mÃºltiples patrones de diseÃ±o para pipelines de datos escalables y mantenibles.

## ğŸ¯ Objetivos del Proyecto

- âœ… Implementar pipelines ETL end-to-end con mÃºltiples fuentes de datos
- âœ… Aplicar arquitectura Medallion para organizaciÃ³n de datos
- âœ… Demostrar expertise en herramientas modernas de procesamiento de datos
- âœ… Implementar buenas prÃ¡cticas de ingenierÃ­a de datos (logging, validaciÃ³n, manejo de errores)
- âœ… Trabajar con diferentes formatos y fuentes: APIs, CSV, bases de datos, web scraping

## ğŸ—ï¸ Arquitectura Medallion Implementada

```mermaid
graph TD
    A[ğŸ”µ BRONZE LAYER<br/>Raw Data] --> B[ğŸ¥ˆ SILVER LAYER<br/>Cleaned Data]
    B --> C[ğŸ¥‡ GOLD LAYER<br/>Business Ready]
    
    A1[Datos originales sin transformar] -.-> A
    A2[Metadata de extracciÃ³n] -.-> A
    A3[Formatos: JSON, CSV, Parquet] -.-> A
    
    B1[Datos limpios y validados] -.-> B
    B2[Tipos correctos] -.-> B
    B3[Duplicados eliminados] -.-> B
    
    C1[Datos agregados] -.-> C
    C2[MÃ©tricas calculadas] -.-> C
    C3[Optimizado para BI/ML] -.-> C
    
    style A fill:#1e3a8a,stroke:#3b82f6,stroke-width:2px,color:#fff
    style B fill:#475569,stroke:#94a3b8,stroke-width:2px,color:#fff
    style C fill:#854d0e,stroke:#fbbf24,stroke-width:2px,color:#fff
```

## ğŸ› ï¸ TecnologÃ­as y Herramientas

### Core Technologies

| TecnologÃ­a | DescripciÃ³n |
|------------|-------------|
| ![Python](https://img.shields.io/badge/Python-3.8+-3776AB?logo=python&logoColor=white) | Lenguaje principal |
| ![Pandas](https://img.shields.io/badge/Pandas-150458?logo=pandas&logoColor=white) | Procesamiento y anÃ¡lisis de datos |
| ![Polars](https://img.shields.io/badge/Polars-CD792C?logo=polars&logoColor=white) | Procesamiento de alto rendimiento |
| ![PyArrow](https://img.shields.io/badge/PyArrow-000000?logo=apache&logoColor=white) | Formato columnar eficiente (Parquet) |
| ![SQLite](https://img.shields.io/badge/SQLite-07405E?logo=sqlite&logoColor=white) | Base de datos relacional |

### Data Extraction

- **Requests** - Consumo de APIs REST
- **Selenium + WebDriver** - Web scraping dinÃ¡mico
- **BeautifulSoup** - Parsing HTML

### Data Quality

- **Logging** - Trazabilidad y debugging
- **Type Hints** - CÃ³digo mÃ¡s robusto y mantenible
- **Error Handling** - Manejo comprehensivo de excepciones

### Visualization

- **Matplotlib** - GrÃ¡ficos estÃ¡ticos
- **Seaborn** - Visualizaciones estadÃ­sticas
- **Plotly** - Dashboards interactivos (opcional)

## ğŸ“ Estructura del Proyecto

```
data-engineering-portfolio/
â”‚
â”œâ”€â”€ ğŸ“Š Pandas_ETL.ipynb                      # ETL complejo con e-commerce data
â”œâ”€â”€ âš¡ ETL_Polars.ipynb                      # ETL de alto rendimiento
â”œâ”€â”€ ğŸ—ï¸ Pyarrow_Ejercicios.ipynb             # Procesamiento columnar
â”œâ”€â”€ ğŸ“ pyarrow_colab.py                      # Script PyArrow standalone
â”œâ”€â”€ ğŸ—„ï¸ Conexion_DB_SQLite_ETL.ipynb         # Pipeline con base de datos
â”œâ”€â”€ ğŸŒ Request_API_ETL_con_API_Pokemon.ipynb # Consumo de APIs REST
â”œâ”€â”€ ğŸ•·ï¸ Webscrapping_Selenium.ipynb          # ExtracciÃ³n desde Wikipedia
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## ğŸš€ Proyectos Implementados

### ğŸ“Š 1. ETL E-Commerce con Pandas

**Archivo:** `Pandas_ETL.ipynb`

**DescripciÃ³n:** Pipeline ETL complejo para anÃ¡lisis de datos de e-commerce con mÃºltiples dimensiones.

**CaracterÃ­sticas:**
- GeneraciÃ³n de datos sintÃ©ticos (Clientes, Productos, Ventas, Inventario)
- Limpieza y normalizaciÃ³n de 5000+ registros
- 12+ visualizaciones avanzadas (heatmaps, box plots, scatter plots)
- AnÃ¡lisis RFM (Recency, Frequency, Monetary)
- DetecciÃ³n de patrones de compra por hora/dÃ­a
- CÃ¡lculo de mÃ©tricas de negocio (ticket promedio, margen de ganancia)

**MÃ©tricas Procesadas:**
- ğŸ§‘â€ğŸ¤â€ğŸ§‘ 1000 clientes
- ğŸ“¦ 200 productos
- ğŸ’° 5000 transacciones
- ğŸ·ï¸ 6 categorÃ­as de productos

---

### âš¡ 2. ETL de Alto Rendimiento con Polars

**Archivo:** `ETL_Polars.ipynb`

**DescripciÃ³n:** ImplementaciÃ³n de ETL utilizando Polars para procesamiento ultrarrÃ¡pido.

**Ventajas de Polars:**
- âš¡ 5-10x mÃ¡s rÃ¡pido que Pandas
- ğŸ’¾ Menor consumo de memoria
- ğŸ”„ Lazy evaluation para optimizaciÃ³n automÃ¡tica
- ğŸ¯ API moderna y expresiva

**Pipeline:**
1. ExtracciÃ³n de 1000 registros de ventas
2. Transformaciones con sintaxis lazy
3. Agregaciones por mÃºltiples dimensiones
4. ExportaciÃ³n a Parquet comprimido

---

### ğŸ—ï¸ 3. Procesamiento Columnar con PyArrow

**Archivos:** `Pyarrow_Ejercicios.ipynb`, `pyarrow_colab.py`

**DescripciÃ³n:** ImplementaciÃ³n de pipeline ETL usando formato Parquet y PyArrow.

**CaracterÃ­sticas Destacadas:**
- ConversiÃ³n entre Pandas y PyArrow
- CompresiÃ³n eficiente (Snappy, GZIP)
- Lectura selectiva de columnas (predicate pushdown)
- Particionamiento de datos
- Comparativa de rendimiento vs CSV

**Benchmark Results:**
- ğŸ“ˆ Parquet 3-5x mÃ¡s rÃ¡pido que CSV
- ğŸ’¾ TamaÃ±o de archivo 60-70% menor
- âš¡ Lectura columnar optimizada

---

### ğŸ—„ï¸ 4. ETL con Base de Datos SQLite

**Archivo:** `Conexion_DB_SQLite_ETL.ipynb`

**DescripciÃ³n:** Pipeline ETL completo integrando base de datos relacional.

**Funcionalidades:**
- CreaciÃ³n y gestiÃ³n de base de datos SQLite
- Ãndices para optimizaciÃ³n de consultas
- Transacciones ACID
- ValidaciÃ³n de calidad de datos
- Consultas SQL complejas (GROUP BY, JOIN, agregaciones)
- Tablas dimensionales agregadas

**Tablas Generadas:**
- `ventas` - Tabla de hechos principal
- `ventas_por_producto` - Agregaciones por producto
- `ventas_por_region` - Agregaciones geogrÃ¡ficas
- `ventas_mensuales` - Series temporales

---

### ğŸŒ 5. Consumo de APIs REST - PokÃ©API

**Archivo:** `Request_API_ETL_con_API_Pokemon.ipynb`

**DescripciÃ³n:** ETL completo consumiendo API pÃºblica con arquitectura Medallion.

**API:** [PokÃ©API](https://pokeapi.co/)

**Pipeline Implementado:**

#### ğŸ”µ Bronze Layer:
- ExtracciÃ³n de 50 PokÃ©mon con rate limiting
- Almacenamiento de JSON raw
- Metadata de extracciÃ³n

#### ğŸ¥ˆ Silver Layer:
4 tablas normalizadas:
- `pokemon_base` - InformaciÃ³n bÃ¡sica
- `pokemon_types` - Tipos de PokÃ©mon
- `pokemon_stats` - EstadÃ­sticas de combate
- `pokemon_abilities` - Habilidades

#### ğŸ¥‡ Gold Layer:
- `pokemon_summary` - Vista consolidada con mÃ©tricas
- `type_analysis` - AnÃ¡lisis estadÃ­stico por tipo
- `stats_pivot` - Tabla pivote de estadÃ­sticas

**CaracterÃ­sticas:**
- â±ï¸ Rate limiting respetuoso con la API
- ğŸ›¡ï¸ Manejo robusto de errores HTTP
- ğŸ”„ Reintentos automÃ¡ticos
- ğŸ“ Logging comprehensivo

---

### ğŸ•·ï¸ 6. Web Scraping con Selenium - Wikipedia

**Archivo:** `Webscrapping_Selenium.ipynb`

**DescripciÃ³n:** ETL de web scraping Ã©tico extrayendo datos de paÃ­ses desde Wikipedia.

**Fuente:** [List of countries by population](https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population)

**ImplementaciÃ³n:**
- Web scraping con Selenium en modo headless
- ValidaciÃ³n de URLs por seguridad
- ExtracciÃ³n de tablas HTML complejas
- Limpieza de datos con expresiones regulares

**CaracterÃ­sticas de Seguridad:**
- ğŸ”’ Solo permite URLs HTTPS
- âœ… Whitelist de dominios (wikipedia.org)
- â±ï¸ Rate limiting respetuoso
- ğŸ¤– User-agent legÃ­timo
- â° Timeout de seguridad

**Transformaciones:**
- Limpieza de nÃºmeros con formato internacional
- ConversiÃ³n de tipos de datos
- CategorizaciÃ³n por poblaciÃ³n
- CÃ¡lculo de mÃ©tricas derivadas
- ValidaciÃ³n de rangos

**Output:**
- CSV y Excel con 50 paÃ­ses
- EstadÃ­sticas descriptivas
- Top 10 paÃ­ses mÃ¡s poblados
- AnÃ¡lisis de porcentaje mundial

## ğŸ’¡ Habilidades TÃ©cnicas Demostradas

<table>
<tr>
<td width="50%">

### Data Engineering
- âœ… DiseÃ±o e implementaciÃ³n de pipelines ETL
- âœ… Arquitectura Medallion (Bronze-Silver-Gold)
- âœ… Data quality y validaciÃ³n
- âœ… Manejo de mÃºltiples fuentes de datos
- âœ… OptimizaciÃ³n de rendimiento

### Python Development
- âœ… POO (ProgramaciÃ³n Orientada a Objetos)
- âœ… Type hints y documentaciÃ³n
- âœ… Manejo de excepciones robusto
- âœ… Logging y debugging
- âœ… Code organization y modularizaciÃ³n

### Data Processing
- âœ… Pandas (manipulaciÃ³n de datos)
- âœ… Polars (alto rendimiento)
- âœ… PyArrow (formato columnar)
- âœ… NumPy (operaciones vectorizadas)

</td>
<td width="50%">

### Databases
- âœ… SQL (queries, joins, agregaciones)
- âœ… SQLite (base de datos embebida)
- âœ… DiseÃ±o de esquemas
- âœ… Ãndices y optimizaciÃ³n

### APIs & Web
- âœ… Consumo de APIs REST
- âœ… Web scraping Ã©tico
- âœ… Selenium automation
- âœ… Rate limiting y retries

### Data Visualization
- âœ… Matplotlib y Seaborn
- âœ… 12+ tipos de grÃ¡ficos
- âœ… Heatmaps y anÃ¡lisis temporal
- âœ… Dashboards analÃ­ticos

</td>
</tr>
</table>

## ğŸ“Š MÃ©tricas del Proyecto

| MÃ©trica | Valor |
|---------|-------|
| ğŸ“ Proyectos Completados | 6 |
| ğŸ’» LÃ­neas de CÃ³digo | 3000+ |
| ğŸ““ Notebooks Jupyter | 6 |
| ğŸ“Š Registros Procesados | 10,000+ |
| ğŸ“ˆ Visualizaciones Creadas | 15+ |
| ğŸ”Œ Fuentes de Datos | 5 (API, CSV, DB, Web, SintÃ©tico) |
| ğŸ“„ Formatos Soportados | JSON, CSV, Parquet, Excel, SQLite |

## ğŸš€ InstalaciÃ³n y Uso

### Requisitos Previos

- Python 3.8 o superior
- pip (gestor de paquetes)
- Google Colab (opcional, para notebooks)

### InstalaciÃ³n

```bash
# Clonar el repositorio
git clone https://github.com/tu-usuario/data-engineering-portfolio.git
cd data-engineering-portfolio

# Crear entorno virtual (recomendado)
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### Dependencias Principales

```txt
pandas>=2.0.0
polars>=0.19.0
pyarrow>=13.0.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
requests>=2.31.0
selenium>=4.15.0
webdriver-manager>=4.0.0
openpyxl>=3.1.0
```

### EjecuciÃ³n

#### OpciÃ³n 1: Jupyter Notebooks Localmente

```bash
jupyter notebook
# Navegar a cualquier archivo .ipynb y ejecutar
```

#### OpciÃ³n 2: Google Colab

1. Subir notebooks a Google Drive
2. Abrir con Google Colab
3. Ejecutar celdas secuencialmente

#### OpciÃ³n 3: Scripts Python

```bash
python pyarrow_colab.py
```

## ğŸ“ˆ Ejemplos de Uso

### ETL BÃ¡sico con Pandas

```python
import pandas as pd

# Extract
df = pd.read_csv('datos_raw.csv')

# Transform
df_clean = df.dropna()
df_clean['total'] = df_clean['cantidad'] * df_clean['precio']

# Load
df_clean.to_csv('datos_procesados.csv', index=False)
```

### ETL con Polars (Alto Rendimiento)

```python
import polars as pl

# Extract y Transform (Lazy)
df = (
    pl.scan_csv('ventas.csv')
    .filter(pl.col('estado') == 'Completado')
    .with_columns([
        (pl.col('cantidad') * pl.col('precio')).alias('total')
    ])
    .collect()
)

# Load
df.write_parquet('ventas_procesadas.parquet')
```

### Consumo de API con Rate Limiting

```python
import requests
import time

def extract_with_rate_limit(api_url, limit=100):
    data = []
    for i in range(limit):
        response = requests.get(f"{api_url}/{i}")
        if response.status_code == 200:
            data.append(response.json())
        time.sleep(0.1)  # Rate limiting
    return data
```

## ğŸ“ Patrones y Mejores PrÃ¡cticas Implementadas

### 1. Separation of Concerns
Cada capa (Bronze, Silver, Gold) tiene responsabilidades claramente definidas.

### 2. DRY (Don't Repeat Yourself)
Funciones reutilizables para operaciones comunes.

### 3. Error Handling

```python
try:
    df = pd.read_csv('file.csv')
except FileNotFoundError:
    logger.error("Archivo no encontrado")
    df = create_sample_data()
except Exception as e:
    logger.error(f"Error inesperado: {e}")
    raise
```

### 4. Logging

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Iniciando ETL...")
logger.warning("Datos faltantes detectados")
logger.error("Error crÃ­tico en transformaciÃ³n")
```

### 5. Type Hints

```python
from typing import List, Dict

def transform_data(df: pd.DataFrame) -> pd.DataFrame:
    """Transforma los datos aplicando reglas de negocio."""
    return df.dropna()
```

### 6. Documentation
Docstrings en formato Google/NumPy para todas las funciones.

## ğŸ”„ Roadmap Futuro

- [ ] Implementar Apache Airflow para orquestaciÃ³n
- [ ] Agregar tests unitarios con pytest
- [ ] Integrar con AWS S3 para almacenamiento cloud
- [ ] Implementar CI/CD con GitHub Actions
- [ ] Crear dashboard interactivo con Streamlit
- [ ] Agregar soporte para Apache Spark
- [ ] Implementar data versioning con DVC
- [ ] Agregar monitoreo con Great Expectations

## ğŸ‘¨â€ğŸ’» Sobre mÃ­

Soy Evaristo Sandoval Gil, desarrollador con formaciÃ³n en desarrollo web y especializaciÃ³n en 
Big Data, IA y Cloud, actualmente enfocado en iniciar mi carrera profesional en el mundo de los datos.

## ğŸ“§ Contacto

Si eres reclutador, technical lead o formas parte de un equipo de datos y quieres comentar este proyecto
o mi perfil profesional, puedes contactarme a travÃ©s de LinkedIn o GitHub.
Si tienes preguntas o sugerencias, no dudes en abrir un issue en este repositorio.

---

<div align="center">

â­ **Si este repositorio te fue Ãºtil, considera darle una estrella!** â­

**Hecho con â¤ï¸ y Python**

</div>
