ðŸ“‹ DescripciÃ³n del Proyecto
Portfolio completo de IngenierÃ­a de Datos que demuestra dominio en procesos ETL (Extract, Transform, Load) utilizando diferentes tecnologÃ­as y metodologÃ­as. Este repositorio implementa la Arquitectura Medallion (Bronze â†’ Silver â†’ Gold) y mÃºltiples patrones de diseÃ±o para pipelines de datos escalables y mantenibles.
ðŸŽ¯ Objetivos del Proyecto

âœ… Implementar pipelines ETL end-to-end con mÃºltiples fuentes de datos
âœ… Aplicar arquitectura Medallion para organizaciÃ³n de datos
âœ… Demostrar expertise en herramientas modernas de procesamiento de datos
âœ… Implementar buenas prÃ¡cticas de ingenierÃ­a de datos (logging, validaciÃ³n, manejo de errores)
âœ… Trabajar con diferentes formatos y fuentes: APIs, CSV, bases de datos, web scraping


ðŸ—ï¸ Arquitectura Medallion Implementada
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BRONZE LAYER (Raw Data)                   â”‚
â”‚  â€¢ Datos en formato original sin transformaciones            â”‚
â”‚  â€¢ Incluye metadata de extracciÃ³n                            â”‚
â”‚  â€¢ Formatos: JSON, CSV, Parquet                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SILVER LAYER (Cleaned Data)                 â”‚
â”‚  â€¢ Datos limpios y validados                                 â”‚
â”‚  â€¢ Tipos de datos correctos                                  â”‚
â”‚  â€¢ Valores nulos manejados                                   â”‚
â”‚  â€¢ Duplicados eliminados                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                GOLD LAYER (Business Ready)                   â”‚
â”‚  â€¢ Datos agregados listos para anÃ¡lisis                     â”‚
â”‚  â€¢ MÃ©tricas calculadas                                       â”‚
â”‚  â€¢ Tablas dimensionales/hechos                               â”‚
â”‚  â€¢ Optimizado para consumo por BI/ML                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ðŸ› ï¸ TecnologÃ­as y Herramientas
Core Technologies

Python 3.8+ - Lenguaje principal
Pandas - Procesamiento y anÃ¡lisis de datos
Polars - Procesamiento de alto rendimiento
PyArrow - Formato columnar eficiente (Parquet)
SQLite - Base de datos relacional

Data Extraction

Requests - Consumo de APIs REST
Selenium + WebDriver - Web scraping dinÃ¡mico
BeautifulSoup - Parsing HTML (implÃ­cito)

Data Quality

Logging - Trazabilidad y debugging
Type Hints - CÃ³digo mÃ¡s robusto y mantenible
Error Handling - Manejo comprehensivo de excepciones

Visualization

Matplotlib - GrÃ¡ficos estÃ¡ticos
Seaborn - Visualizaciones estadÃ­sticas
Plotly - Dashboards interactivos (opcional)


ðŸ“ Estructura del Proyecto
data-engineering-portfolio/
â”‚
â”œâ”€â”€ 01_Pandas_ETL/
â”‚   â”œâ”€â”€ Pandas_ETL.ipynb                    # ETL complejo con e-commerce data
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ 02_Polars_ETL/
â”‚   â”œâ”€â”€ ETL_Polars.ipynb                    # ETL de alto rendimiento
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ 03_PyArrow_ETL/
â”‚   â”œâ”€â”€ Pyarrow_Ejercicios.ipynb            # Procesamiento columnar
â”‚   â”œâ”€â”€ pyarrow_colab.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ 04_SQLite_ETL/
â”‚   â”œâ”€â”€ Conexion_DB_SQLite_ETL.ipynb        # Pipeline con base de datos
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ 05_API_ETL/
â”‚   â”œâ”€â”€ Request_API_ETL_con_API_Pokemon.ipynb  # Consumo de APIs REST
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ 06_WebScraping_ETL/
â”‚   â”œâ”€â”€ Webscrapping_Selenium.ipynb         # ExtracciÃ³n desde Wikipedia
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ Entregable 02 Data Engineer Python.docx  # DocumentaciÃ³n tÃ©cnica
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md

ðŸš€ Proyectos Implementados
1. ðŸ“Š ETL E-Commerce con Pandas
Archivo: Pandas_ETL.ipynb
DescripciÃ³n: Pipeline ETL complejo para anÃ¡lisis de datos de e-commerce con mÃºltiples dimensiones.
CaracterÃ­sticas:

GeneraciÃ³n de datos sintÃ©ticos (Clientes, Productos, Ventas, Inventario)
Limpieza y normalizaciÃ³n de 5000+ registros
12+ visualizaciones avanzadas (heatmaps, box plots, scatter plots)
AnÃ¡lisis RFM (Recency, Frequency, Monetary)
DetecciÃ³n de patrones de compra por hora/dÃ­a
CÃ¡lculo de mÃ©tricas de negocio (ticket promedio, margen de ganancia)

MÃ©tricas Procesadas:

1000 clientes
200 productos
5000 transacciones
6 categorÃ­as de productos


2. âš¡ ETL de Alto Rendimiento con Polars
Archivo: ETL_Polars.ipynb
DescripciÃ³n: ImplementaciÃ³n de ETL utilizando Polars para procesamiento ultrarrÃ¡pido.
Ventajas de Polars:

5-10x mÃ¡s rÃ¡pido que Pandas
Menor consumo de memoria
Lazy evaluation para optimizaciÃ³n automÃ¡tica
API moderna y expresiva

Pipeline:

ExtracciÃ³n de 1000 registros de ventas
Transformaciones con sintaxis lazy
Agregaciones por mÃºltiples dimensiones
ExportaciÃ³n a Parquet comprimido


3. ðŸ—ï¸ Procesamiento Columnar con PyArrow
Archivos: Pyarrow_Ejercicios.ipynb, pyarrow_colab.py
DescripciÃ³n: ImplementaciÃ³n de pipeline ETL usando formato Parquet y PyArrow.
CaracterÃ­sticas Destacadas:

ConversiÃ³n entre Pandas y PyArrow
CompresiÃ³n eficiente (Snappy, GZIP)
Lectura selectiva de columnas (predicate pushdown)
Particionamiento de datos
Comparativa de rendimiento vs CSV

Benchmark Results:

Parquet 3-5x mÃ¡s rÃ¡pido que CSV
TamaÃ±o de archivo 60-70% menor
Lectura columnar optimizada


4. ðŸ—„ï¸ ETL con Base de Datos SQLite
Archivo: Conexion_DB_SQLite_ETL.ipynb
DescripciÃ³n: Pipeline ETL completo integrando base de datos relacional.
Funcionalidades:

CreaciÃ³n y gestiÃ³n de base de datos SQLite
Ãndices para optimizaciÃ³n de consultas
Transacciones ACID
ValidaciÃ³n de calidad de datos
Consultas SQL complejas (GROUP BY, JOIN, agregaciones)
Tablas dimensionales agregadas

Tablas Generadas:

ventas - Tabla de hechos principal
ventas_por_producto - Agregaciones por producto
ventas_por_region - Agregaciones geogrÃ¡ficas
ventas_mensuales - Series temporales


5. ðŸŒ Consumo de APIs REST - PokÃ©API
Archivo: Request_API_ETL_con_API_Pokemon.ipynb
DescripciÃ³n: ETL completo consumiendo API pÃºblica con arquitectura Medallion.
API: PokÃ©API
Pipeline Implementado:
Bronze Layer:

ExtracciÃ³n de 50 PokÃ©mon con rate limiting
Almacenamiento de JSON raw
Metadata de extracciÃ³n

Silver Layer:

4 tablas normalizadas:

pokemon_base - InformaciÃ³n bÃ¡sica
pokemon_types - Tipos de PokÃ©mon
pokemon_stats - EstadÃ­sticas de combate
pokemon_abilities - Habilidades



Gold Layer:

pokemon_summary - Vista consolidada con mÃ©tricas
type_analysis - AnÃ¡lisis estadÃ­stico por tipo
stats_pivot - Tabla pivote de estadÃ­sticas

CaracterÃ­sticas:

Rate limiting respetuoso con la API
Manejo robusto de errores HTTP
Reintentos automÃ¡ticos
Logging comprehensivo


6. ðŸ•·ï¸ Web Scraping con Selenium - Wikipedia
Archivo: Webscrapping_Selenium.ipynb
DescripciÃ³n: ETL de web scraping Ã©tico extrayendo datos de paÃ­ses desde Wikipedia.
Fuente: List of countries by population
ImplementaciÃ³n:

Web scraping con Selenium en modo headless
ValidaciÃ³n de URLs por seguridad
ExtracciÃ³n de tablas HTML complejas
Limpieza de datos con expresiones regulares

CaracterÃ­sticas de Seguridad:

Solo permite URLs HTTPS
Whitelist de dominios (wikipedia.org)
Rate limiting respetuoso
User-agent legÃ­timo
Timeout de seguridad

Transformaciones:

Limpieza de nÃºmeros con formato internacional
ConversiÃ³n de tipos de datos
CategorizaciÃ³n por poblaciÃ³n
CÃ¡lculo de mÃ©tricas derivadas
ValidaciÃ³n de rangos

Output:

CSV y Excel con 50 paÃ­ses
EstadÃ­sticas descriptivas
Top 10 paÃ­ses mÃ¡s poblados
AnÃ¡lisis de porcentaje mundial


ðŸ’¡ Habilidades TÃ©cnicas Demostradas
Data Engineering

âœ… DiseÃ±o e implementaciÃ³n de pipelines ETL
âœ… Arquitectura Medallion (Bronze-Silver-Gold)
âœ… Data quality y validaciÃ³n
âœ… Manejo de mÃºltiples fuentes de datos
âœ… OptimizaciÃ³n de rendimiento

Python Development

âœ… POO (ProgramaciÃ³n Orientada a Objetos)
âœ… Type hints y documentaciÃ³n
âœ… Manejo de excepciones robusto
âœ… Logging y debugging
âœ… Code organization y modularizaciÃ³n

Data Processing

âœ… Pandas (manipulaciÃ³n de datos)
âœ… Polars (alto rendimiento)
âœ… PyArrow (formato columnar)
âœ… NumPy (operaciones vectorizadas)

Databases

âœ… SQL (queries, joins, agregaciones)
âœ… SQLite (base de datos embebida)
âœ… DiseÃ±o de esquemas
âœ… Ãndices y optimizaciÃ³n

APIs & Web

âœ… Consumo de APIs REST
âœ… Web scraping Ã©tico
âœ… Selenium automation
âœ… Rate limiting y retries

Data Visualization

âœ… Matplotlib y Seaborn
âœ… 12+ tipos de grÃ¡ficos
âœ… Heatmaps y anÃ¡lisis temporal
âœ… Dashboards analÃ­ticos


ðŸ“Š MÃ©tricas del Proyecto
MÃ©tricaValorProyectos Completados6LÃ­neas de CÃ³digo3000+Notebooks Jupyter8Registros Procesados10,000+Visualizaciones Creadas15+Fuentes de Datos5 (API, CSV, DB, Web, SintÃ©tico)Formatos SoportadosJSON, CSV, Parquet, Excel, SQLite

ðŸš€ InstalaciÃ³n y Uso
Requisitos Previos

Python 3.8 o superior
pip (gestor de paquetes)
Google Colab (opcional, para notebooks)

InstalaciÃ³n
bash# Clonar el repositorio
git clone https://github.com/tu-usuario/data-engineering-portfolio.git
cd data-engineering-portfolio

# Crear entorno virtual (recomendado)
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
Dependencias Principales
txtpandas>=2.0.0
polars>=0.19.0
pyarrow>=13.0.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
requests>=2.31.0
selenium>=4.15.0
webdriver-manager>=4.0.0
openpyxl>=3.1.0
EjecuciÃ³n
OpciÃ³n 1: Jupyter Notebooks Localmente
bashjupyter notebook
# Navegar a cualquier archivo .ipynb y ejecutar
OpciÃ³n 2: Google Colab

Subir notebooks a Google Drive
Abrir con Google Colab
Ejecutar celdas secuencialmente

OpciÃ³n 3: Scripts Python
bashpython pyarrow_colab.py

ðŸ“ˆ Ejemplos de Uso
ETL BÃ¡sico con Pandas
pythonimport pandas as pd

# Extract
df = pd.read_csv('datos_raw.csv')

# Transform
df_clean = df.dropna()
df_clean['total'] = df_clean['cantidad'] * df_clean['precio']

# Load
df_clean.to_csv('datos_procesados.csv', index=False)
ETL con Polars (Alto Rendimiento)
pythonimport polars as pl

# Extract y Transform (Lazy)
df = (
    pl.scan_csv('ventas.csv')
    .filter(pl.col('estado') == 'Completado')
    .with_columns([
        (pl.col('cantidad') * pl.col('precio')).alias('total')
    ])
    .collect()
)

# Load
df.write_parquet('ventas_procesadas.parquet')
Consumo de API con Rate Limiting
pythonimport requests
import time

def extract_with_rate_limit(api_url, limit=100):
    data = []
    for i in range(limit):
        response = requests.get(f"{api_url}/{i}")
        if response.status_code == 200:
            data.append(response.json())
        time.sleep(0.1)  # Rate limiting
    return data

ðŸŽ“ Patrones y Mejores PrÃ¡cticas Implementadas
1. Separation of Concerns
Cada capa (Bronze, Silver, Gold) tiene responsabilidades claramente definidas.
2. DRY (Don't Repeat Yourself)
Funciones reutilizables para operaciones comunes.
3. Error Handling
pythontry:
    df = pd.read_csv('file.csv')
except FileNotFoundError:
    logger.error("Archivo no encontrado")
    df = create_sample_data()
except Exception as e:
    logger.error(f"Error inesperado: {e}")
    raise
4. Logging
pythonimport logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info("Iniciando ETL...")
logger.warning("Datos faltantes detectados")
logger.error("Error crÃ­tico en transformaciÃ³n")
5. Type Hints
pythonfrom typing import List, Dict

def transform_data(df: pd.DataFrame) -> pd.DataFrame:
    """Transforma los datos aplicando reglas de negocio."""
    return df.dropna()
6. Documentation
Docstrings en formato Google/NumPy para todas las funciones.

ðŸ”„ Roadmap Futuro

 Implementar Apache Airflow para orquestaciÃ³n
 Agregar tests unitarios con pytest
 Integrar con AWS S3 para almacenamiento cloud
 Implementar CI/CD con GitHub Actions
 Crear dashboard interactivo con Streamlit
 Agregar soporte para Apache Spark
 Implementar data versioning con DVC
 Agregar monitoreo con Great Expectations
